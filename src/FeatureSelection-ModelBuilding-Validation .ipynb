{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import functools as F\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "#from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Imputer as SimpleImputer\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "#from sklearn.grid_search import GridSearchCV #Perforing grid search\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "#Bayesian optimization for model parameters\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd3fbaa1674dfc4b868ad3b8aaa7b3e3eae37fd3"
   },
   "source": [
    "**Standard Util Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68bcbad122a503d6877b8fb2c7cd08a2861f33a6"
   },
   "outputs": [],
   "source": [
    "def dataImputation(orig_df):\n",
    "    df = orig_df.copy()\n",
    "    # make new columns indicating what will be imputed\n",
    "    cols_with_missing = (col for col in df.columns if df[col].isnull().any())\n",
    "    #for col in cols_with_missing:\n",
    "    #    df[col + '_was_missing'] = df[col].isnull()\n",
    "    # Imputation\n",
    "    my_imputer = SimpleImputer()\n",
    "    df = pd.DataFrame(my_imputer.fit_transform(df),columns=df.columns.values)\n",
    "    \n",
    "    \n",
    "    print(df.columns)\n",
    "    #print(orig_df.columns)\n",
    "    return df\n",
    "\n",
    "#dataImputation(application_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(clf, X_train, y_train=None, \n",
    "                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n",
    "    '''\n",
    "    plot feature importances of a tree-based sklearn estimator\n",
    "    \n",
    "    Note: X_train and y_train are pandas DataFrames\n",
    "    \n",
    "    Note: Scikit-plot is a lovely package but I sometimes have issues\n",
    "              1. flexibility/extendibility\n",
    "              2. complicated models/datasets\n",
    "          But for many situations Scikit-plot is the way to go\n",
    "          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        clf         (sklearn estimator) if not fitted, this routine will fit it\n",
    "        \n",
    "        X_train     (pandas DataFrame)\n",
    "        \n",
    "        y_train     (pandas DataFrame)  optional\n",
    "                                        required only if clf has not already been fitted \n",
    "        \n",
    "        top_n       (int)               Plot the top_n most-important features\n",
    "                                        Default: 10\n",
    "                                        \n",
    "        figsize     ((int,int))         The physical size of the plot\n",
    "                                        Default: (8,8)\n",
    "        \n",
    "        print_table (boolean)           If True, print out the table of feature importances\n",
    "                                        Default: False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        the pandas dataframe with the features and their importance\n",
    "        \n",
    "    Author\n",
    "    ------\n",
    "        George Fisher\n",
    "    '''\n",
    "    \n",
    "    __name__ = \"plot_feature_importances\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy  as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    from xgboost.core     import XGBoostError\n",
    "    from lightgbm.sklearn import LightGBMError\n",
    "    \n",
    "    try: \n",
    "        if not hasattr(clf, 'feature_importances_'):\n",
    "            clf.fit(X_train.values, y_train.values.ravel())\n",
    "\n",
    "            if not hasattr(clf, 'feature_importances_'):\n",
    "                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n",
    "                                    format(clf.__class__.__name__))\n",
    "                \n",
    "    except (XGBoostError, LightGBMError, ValueError):\n",
    "        clf.fit(X_train.values, y_train.values.ravel())\n",
    "            \n",
    "    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n",
    "    feat_imp['feature'] = X_train.columns\n",
    "    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n",
    "    feat_imp = feat_imp.iloc[:top_n]\n",
    "    \n",
    "    feat_imp.sort_values(by='importance', inplace=True)\n",
    "    feat_imp = feat_imp.set_index('feature', drop=True)\n",
    "    feat_imp.plot.barh(title=title, figsize=figsize)\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.show()\n",
    "    \n",
    "    if print_table:\n",
    "        from IPython.display import display\n",
    "        print(\"Top {} features in descending order of importance\".format(top_n))\n",
    "        display(feat_imp.sort_values(by='importance', ascending=False))\n",
    "        \n",
    "    return feat_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_final = pd.read_csv('../input/train_final.csv')\n",
    "test_final  = pd.read_csv('../input/test_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6704ed072dc870020b580189258170c861277b62"
   },
   "source": [
    "**Below Box is for Bayesian Optimization for Model Hyperparameter Tuning\n",
    "Note- It is very resource and time consuming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldValidation(train, features, xgbParams, numRounds, nFolds, target='TARGET'):  \n",
    "   kf = KFold( n_splits = nFolds, shuffle = True).split(train)\n",
    "   fold_score=[]\n",
    "   for train_index, cv_index in kf:\n",
    "      # split train/validation\n",
    "      X_train, X_valid = train[features].as_matrix()[train_index], train[features].as_matrix()[cv_index]\n",
    "      y_train, y_valid = (train[target].as_matrix()[train_index]), (train[target].as_matrix()[cv_index])\n",
    "      dtrain = xgb.DMatrix(X_train, y_train) \n",
    "      dvalid = xgb.DMatrix(X_valid, y_valid)         \n",
    "      watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "      gbm = xgb.train(xgbParams, dtrain, numRounds, evals = watchlist, early_stopping_rounds = 100)\n",
    "      score = gbm.best_score\n",
    "      fold_score.append(score)\n",
    "   return np.mean(fold_score)\n",
    "\n",
    "def xgbCv(train, features, numRounds, eta, gamma, maxDepth, minChildWeight, subsample, colSample,scale_pos_weight,n_estimators):   \n",
    "   # prepare xgb parameters \n",
    "   params = {\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"booster\" : \"gbtree\",\n",
    "            \"eval_metric\": \"auc\",            \n",
    "            \"tree_method\": 'auto',\n",
    "            \"silent\": 1,\n",
    "            \"eta\": eta, \n",
    "            \"max_depth\": int(maxDepth),\n",
    "            \"min_child_weight\" : minChildWeight,\n",
    "            \"subsample\": subsample, \n",
    "            \"colsample_bytree\": colSample,             \n",
    "            \"gamma\": gamma,\n",
    "           \"scale_pos_weight\": scale_pos_weight\n",
    "   }\n",
    "   cvScore = kFoldValidation(train, features, params, int(numRounds), nFolds = 3)\n",
    "   print('CV score: {:.6f}'.format(cvScore)) \n",
    "   return -1.0 * cvScore   # invert the cv score to let bayopt maximize\n",
    "\n",
    "def bayesOpt(train, features):\n",
    "   ranges = {            \n",
    "              'numRounds': (1000, 5000),\n",
    "              'eta': (0.001, 0.3),\n",
    "              'gamma': (0, 25),\n",
    "              'maxDepth': (1, 10),\n",
    "              'minChildWeight': (0, 10),\n",
    "              'subsample': (0, 1),\n",
    "              'colSample': (0, 1),\n",
    "              'scale_pos_weight':(1,50),\n",
    "              'n_estimators': (100,400)\n",
    "   }   \n",
    "   # proxy through a lambda to be able to pass train and features\n",
    "   optFunc = lambda numRounds, eta, gamma, maxDepth, minChildWeight, subsample, colSample, scale_pos_weight, n_estimators: xgbCv(train, features, numRounds, eta, gamma, maxDepth, minChildWeight, subsample, colSample,scale_pos_weight,n_estimators)\n",
    "   bo = BayesianOptimization(optFunc, ranges)\n",
    "   bo.maximize(init_points = 50, n_iter = 5, kappa = 2, acq = \"ei\", xi = 0.0)   \n",
    "   bestMAE = round((-1.0 * bo.res['max']['max_val']), 6)\n",
    "   print(\"\\n Best auc found: %f\" % bestMAE)\n",
    "   print(\"\\n Parameters: %s\" % bo.res['max']['max_params'])\n",
    "\n",
    "target=\"TARGET\"\n",
    "\n",
    "predictors = [x for x in train_final.columns.values if x not in [target,'SK_ID_CURR']]\n",
    "print(predictors)\n",
    "bayesOpt(train_final,predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34c3d271b75d222f5e0a6f561f4790631eee4ae4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''This code will be commented when code will bw committed so that it runs faster'''\n",
    "model_selection = xgb.XGBClassifier(silent=False, \n",
    "                      scale_pos_weight=15,\n",
    "                      learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=300, \n",
    "                      reg_alpha = 1.2,\n",
    "                      min_child_weight=1,    \n",
    "                      max_delta_step=1,                         \n",
    "                      max_depth=5,\n",
    "                      eval_metric='auc',   \n",
    "                      gamma=1,\n",
    "                      early_stopping_rounds= 200)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "train_selection = dataImputation(application_train)\n",
    "predictors_selection = [x for x in train_selection.columns.values if x not in ['TARGET','SK_ID_CURR']]\n",
    "embeded_lgb_selector = SelectFromModel(model_selection, threshold='1.25*median')\n",
    "embeded_lgb_selector.fit(train_selection[predictors_selection],train_selection['TARGET'])\n",
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = train_selection[predictors_selection].loc[:,embeded_lgb_support].columns.tolist()\n",
    "print(embeded_lgb_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0ce3d8f633ccf29404a954d55c06f3a19515b72",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tried with multiple models, but selected XGBOOST as final model. Selected model parameters from above tuning\n",
    "gmb0 = xgb.XGBClassifier(silent=False, \n",
    "                      scale_pos_weight=15,\n",
    "                      learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=300, \n",
    "                      reg_alpha = 1.2,\n",
    "                      min_child_weight=1,    \n",
    "                      max_delta_step=1,                         \n",
    "                      max_depth=5,\n",
    "                      eval_metric='auc',   \n",
    "                      gamma=1,\n",
    "                      early_stopping_rounds= 200).fit(train_final[predictors], train_final[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc63ed4e0d8d63f6a22e5dcea655737c66d6a687"
   },
   "source": [
    "**Evaluating Model  With Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "574c87798d44aba093ba2033f34be237d701fc81"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = xgb.XGBClassifier(silent=True, \n",
    "                      scale_pos_weight=15,\n",
    "                      learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=300, \n",
    "                      reg_alpha = 1.2,\n",
    "                      min_child_weight=1,    \n",
    "                      max_delta_step=1,                         \n",
    "                      max_depth=5,\n",
    "                      eval_metric='auc',   \n",
    "                      gamma=1,\n",
    "                      early_stopping_rounds= 200)\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=7)\n",
    "results = cross_val_score(model,train_final[predictors], train_final[target], cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risk Score**: Assigning a risk against each customer who has applied for loan. The sore has range between 0 & 1, score towards 1 mean higher risk of default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "926dd089b6373c431fd279fc68cbae6e51a32de8"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Below Feature are selected from Feature selection module, \n",
    "if want to train on whole set of features then uncomment 1st line'''\n",
    "#test_predictors = [x for x in test_final.columns.values if x not in ['SK_ID_CURR']]\n",
    "test_predictors = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'LIVINGAREA_AVG', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'TOTALAREA_MODE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_3', 'NEW_CREDIT_TO_ANNUITY_RATIO', 'NEW_CREDIT_TO_GOODS_RATIO', 'ANNUITY_TO_INCOME', 'CREDIT_PER_PERS', 'CREDIT_PER_CHILD', 'CREDIT_PER_NO_CHILD', 'NEW_SOURCES_PROD', 'NEW_EXT_SOURCES_MEAN', 'NEW_SCORES_STD', 'NEW_CAR_TO_BIRTH_RATIO', 'NEW_CAR_TO_EMPLOY_RATIO', 'NEW_PHONE_TO_BIRTH_RATIO', 'NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER', 'NEW_CREDIT_TO_INCOME_RATIO', 'INCOME_BY_DAYS_EMP', 'AVG_EXT_SOURCE', 'DAYS_EMP_TO_BIRTH_RATIO', 'AMT_ANNUITY_TO_DAYS_EMP_RATIO', 'AMT_ANNUITY_TO_DAYS_BIRTH_RATIO', 'AMT_INC_TO_DAYS_BIRTH_RATIO', 'AMT_GOODS_PER_PERS', 'AMT_ANNUITY_PER_PERS', 'BUREAU_NET_SUM_POST_PROP', 'BUREAU_NET_PROD_POST_PROP', 'BUREAU_NET_SUM_AMT_POST_PROP', 'BUREAU_NET_PROD_AMT_POST_PROP', 'DAYS_DIFF_MEAN', 'BUREAU_DAY_CREDIT_BY_CNT_PROLONG_min', 'BUREAU_DAY_CREDIT_BY_CNT_PROLONG_max', 'BUREAU_DAY_CREDIT_BY_CNT_PROLONG_mean', 'BUREAU_DAY_CREDIT_BY_CNT_PROLONG_count', 'CNT_CREDIT_PROLONG_count', 'PREV_REFUSED_RATIO_CREDIT_APPLICATION', 'PREV_NOT_REFUSED_RATIO_CREDIT_APPLICATION', 'PREV_REFUSED_RATIO_ANNUITY_APPLICATION', 'PREV_NOT_REFUSED_RATIO_ANNUITY_APPLICATION', 'PREV_NOT_REFUSED_RATIO_AMT_GOODS_PRICE_CREDIT', 'PREV_REFUSED_RATIO_AMT_GOODS_PRICE_CREDIT', 'PREV_NOT_REFUSED_RATIO_GOODS_ANNUITY', 'PREV_REFUSED_RATIO_GOODS_ANNUITY', 'AMT_INSTALMENT_min', 'AMT_INSTALMENT_max', 'AMT_INSTALMENT_sum', 'AMT_INSTALMENT_mean', 'AMT_PAYMENT_min', 'AMT_PAYMENT_max', 'AMT_PAYMENT_sum', 'AMT_PAYMENT_mean', 'AMT_INSTALMENT_AMT_PAYMENT_DECAY_DIFF_min', 'AMT_INSTALMENT_AMT_PAYMENT_DECAY_DIFF_max', 'AMT_INSTALMENT_AMT_PAYMENT_DECAY_DIFF_sum', 'AMT_INSTALMENT_AMT_PAYMENT_DECAY_DIFF_mean', 'AMT_PAYMENT_DECAY_min', 'AMT_PAYMENT_DECAY_sum', 'DAYS_ENTRY_PAYMENT_min', 'DAYS_ENTRY_PAYMENT_max', 'DAYS_ENTRY_PAYMENT_sum', 'DAYS_ENTRY_PAYMENT_mean', 'DAYS_ENTRY_PAYMENT_var', 'DAYS_ENTRY_PAYMENT_std', 'DAYS_INSTALMENT_min', 'DAYS_INSTALMENT_max', 'DAYS_INSTALMENT_sum', 'DAYS_INSTALMENT_mean', 'DAYS_INSTALMENT_var', 'DAYS_INSTALMENT_std', 'NUM_INSTALMENT_VERSION_sum', 'NUM_INSTALMENT_VERSION_mean', 'NUM_INSTALMENT_VERSION_var', 'DAYS_INSTALMENT_DAYS_ENTRY_PAYMENT_DIFF_min', 'DAYS_INSTALMENT_DAYS_ENTRY_PAYMENT_DIFF_sum', 'DAYS_INSTALMENT_DAYS_ENTRY_PAYMENT_DIFF_mean', 'DAYS_INSTALMENT_DAYS_ENTRY_PAYMENT_DIFF_var', 'DAYS_INSTALMENT_DAYS_ENTRY_PAYMENT_DIFF_std', 'AMT_INSTALMENT_AMT_PAYMENT_DIFF_max', 'AMT_INSTALMENT_AMT_PAYMENT_DIFF_sum', 'AMT_INSTALMENT_AMT_PAYMENT_DIFF_mean', 'PREV_NON_CASH_TOTAL_DEBT_TO_AMT_CREDIT_min', 'PREV_NON_CASH_TOTAL_DEBT_TO_AMT_CREDIT_max', 'PREV_NON_CASH_TOTAL_DEBT_TO_AMT_CREDIT_sum', 'PREV_NON_CASH_TOTAL_DEBT_TO_AMT_CREDIT_mean', 'PREV_NON_CASH_TOTAL_DEBT_TO_AMT_CREDIT_var', 'PREV_NON_CASH_TOTAL_DEBT_TO_AMT_CREDIT_std', 'PREV_CASH_TOTAL_DEBT_TO_AMT_CREDIT_min', 'PREV_CASH_TOTAL_DEBT_TO_AMT_CREDIT_max', 'PREV_CASH_TOTAL_DEBT_TO_AMT_CREDIT_sum', 'PREV_CASH_TOTAL_DEBT_TO_AMT_CREDIT_mean', 'PREV_CASH_TOTAL_DEBT_TO_AMT_CREDIT_var', 'DAYS_DECISION_min', 'DAYS_DECISION_max', 'DAYS_DECISION_mean', 'DAYS_DECISION_var']\n",
    "#test_predictors = ['FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_9', 'HOUSETYPE_MODE', 'EMERGENCYSTATE_MODE', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_11', 'FLAG_EMP_PHONE', 'FLAG_PHONE', 'REG_REGION_NOT_WORK_REGION', 'LIVE_CITY_NOT_WORK_CITY', 'FLAG_EMAIL', 'FLAG_DOCUMENT_14', 'REG_CITY_NOT_WORK_CITY', 'ELEVATORS_MEDI', 'CHILDREN_RATIO', 'ELEVATORS_MODE', 'FLAG_DOCUMENT_13', 'CNT_CHILDREN', 'NONLIVINGAPARTMENTS_MEDI', 'CNT_FAM_MEMBERS', 'ENTRANCES_MEDI', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'FLAG_OWN_REALTY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'FLOORSMAX_MEDI', 'FONDKAPREMONT_MODE', 'FLOORSMAX_MODE', 'ELEVATORS_AVG', 'FLAG_DOCUMENT_18', 'REGION_RATING_CLIENT', 'ENTRANCES_MODE', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI', 'CNT_NO_CHILD', 'OBS_30_CNT_SOCIAL_CIRCLE', 'FLAG_DOCUMENT_16', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MODE', 'YEARS_BUILD_MEDI', 'AMT_REQ_CREDIT_BUREAU_DAY', 'YEARS_BUILD_MODE', 'FLAG_OWN_CAR', 'REG_CITY_NOT_LIVE_CITY', 'NAME_HOUSING_TYPE', 'NAME_TYPE_SUITE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'LIVINGAPARTMENTS_MODE', 'FLOORSMIN_AVG', 'ENTRANCES_AVG', 'BUREAU_LOAN_TYPES', 'WALLSMATERIAL_MODE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'FLOORSMAX_AVG', 'NONLIVINGAREA_MEDI', 'APARTMENTS_MODE', 'APARTMENTS_MEDI', 'AMT_REQ_CREDIT_BUREAU_MON', 'LIVINGAPARTMENTS_MEDI', 'NAME_CONTRACT_TYPE', 'NAME_INCOME_TYPE', 'LANDAREA_MEDI', 'FLAG_DOCUMENT_3', 'BASEMENTAREA_MODE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'LIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_MODE', 'WEEKDAY_APPR_PROCESS_START', 'YEARS_BUILD_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'NONLIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI', 'LANDAREA_MODE', 'YEARS_BEGINEXPLUATATION_MEDI', 'BASEMENTAREA_MEDI', 'AMT_REQ_CREDIT_BUREAU_QRT', 'COMMONAREA_MODE', 'COMMONAREA_AVG', 'LIVINGAREA_AVG', 'FLAG_WORK_PHONE', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'COMMONAREA_MEDI', 'PREV_NFLAG_INSURED_SUM', 'YEARS_BEGINEXPLUATATION_MODE', 'APARTMENTS_AVG', 'LANDAREA_AVG', 'TOTALAREA_MODE', 'BASEMENTAREA_AVG', 'NEW_INC_PER_CHLD', 'NAME_FAMILY_STATUS', 'BUREAU_LOAN_COUNT', 'REGION_RATING_CLIENT_W_CITY', 'OCCUPATION_TYPE', 'DAYS_DIFF_STD', 'NEW_INCOME_PER_PERS', 'BUREAU_LOAN_PER_TYPE', 'HOUR_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'AMT_INCOME_TOTAL', 'CREDIT_PER_CHILD', 'NEW_CAR_TO_BIRTH_RATIO', 'NAME_EDUCATION_TYPE', 'REGION_POPULATION_RELATIVE', 'CODE_GENDER', 'DAYS_DIFF_MEAN', 'OWN_CAR_AGE', 'PREV_REFUSED_RATIO_ANNUITY_APPLICATION', 'DAYS_DIFF_RATIO', 'CREDIT_PER_PERS', 'NEW_PHONE_TO_BIRTH_RATIO', 'NEW_SCORES_STD', 'CREDIT_PER_NO_CHILD', 'AMT_CREDIT', 'DAYS_LAST_PHONE_CHANGE', 'NEW_CAR_TO_EMPLOY_RATIO', 'NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER', 'PREV_REFUSED_RATIO_CREDIT_APPLICATION', 'NEW_CREDIT_TO_INCOME_RATIO', 'NEW_CREDIT_TO_GOODS_RATIO', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED', 'AMT_GOODS_PRICE', 'DAYS_ID_PUBLISH', 'AMT_ANNUITY', 'NEW_SOURCES_PROD', 'PREV_NOT_REFUSED_RATIO_ANNUITY_APPLICATION', 'INCOME_BY_DAYS_EMP', 'ANNUITY_TO_INCOME', 'PREV_NOT_REFUSED_RATIO_CREDIT_APPLICATION', 'EXT_SOURCE_2', 'EXT_SOURCE_1', 'NEW_EXT_SOURCES_MEAN', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'NEW_CREDIT_TO_ANNUITY_RATIO']\n",
    "score = np.array(gmb0.predict_proba(test_final[test_predictors]))\n",
    "test_final['TARGET'] = score[:,0]\n",
    "cust_with_score = test_final[['SK_ID_CURR','TARGET']]\n",
    "cust_with_score.to_csv('submission_post5.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
